---
layout: default
---
## Employment History

### Autodesk

> Contractor Data Engineer, Remote Toronto, Canada (Agu 2022 - Nov 2023)

* As a Data Engineer at the Halley Operations team, I worked with AutoDesk product insights.

* I was on an amazing team tasked with taking the operations performance to the next level by delivering *quality insight deployments* and incrementing the amount of insights available to AutoCAD and Revit end users.

### The Walt Disney Company

> Contractor Data Engineer, Remote Buenos Aires, Argentina (Agu 2021 - Nov 2021)

* In my role, I was tasked with developing a proof of concept using Google Cloud Platform to ingest data from two sources: **Google Trends (GT)** and **Adobe Analytics data feed (ADF)**.

* I built a **Python script** to ingest data from Google Trends and created a basic dashboard using **Looker**. Additionally, I had to leverage **Cloud Functions** to preprocess **ADF compressed files**.


* As a result of these actions, the proof of concept was successfully implemented, demonstrating the capability to ingest data from both Google Trends and Adobe Analytics efficiently. The Python program and Looker dashboard enhanced **data accessibility** and visualization, while the integration of Cloud Functions streamlined the preprocessing of ADF compressed files.

> Contractor Data Engineer, Remote Burbank, CA, USA (Nov 2021 - Aug 2022)

* As a Data Engineer at Disney Mediaâ€™s Descriptive Metadata Management, I was tasked with managing metadata using tools deployed on the AWS cloud. The metadata was stored in **AWS Neptune** for graph data, and status and temporary data were stored in **DynamoDB**.

* My responsibilities included fixing broken records processed by **data pipelines** involving upstream/downstream systems. This encompassed tasks such as resource tagging, video search, and the utilization of message queues to exchange data.

* To address the broken records and improve **data quality**, I developed updates for the reporting system using Java. Additionally, I built a process to clean tagging working data using Python, **AWS Lambda functions**, and **DynamoDB**.

* The implemented updates and processes resulted in a more robust and reliable metadata management system. The data pipeline involving various systems was improved, leading to the successful resolution of broken records and streamlined data exchange processes.

### CH Robinson

> Contractor Data Engineer, Remote Eden Prairie, MN, USA (Feb 2021 - Jul 2021)

* Performed Kafka topics migration tasks from on-prem Kafka servers to Microsoft Azure.

* Developed **Airflow** Dags to serve data to CHR Contractual service.

* Executed a migration from on-prem **MongoDB** datasets to MongoDB Atlas (cloud).

### Azlo / BBVA

> Contractor Data Engineer, Remote CA, USA (Feb 2020 - Feb 2021)

* I have been involved in designing, implementing and orchestrating data workflows tailored to meet specific business analytic requirements.

* Created scalable data pipelines, selecting appropriate **data storage solution**, designing **data models and schemas**, and ensuring efficient data workflow in AWS.

* Leveraged on best practices and data processing technology (Spark) to design comprehensive data models and schemas, and ensured the efficient flow of data to deliver customer value to the analytic team.

### Banco Falabella

> Contractor Data Engineer, Santiago Chile (Jul 2019 - dec 2019)

* The organization faced the need to define **data strategies** and develop the roadmap for achieving data migration-related goals to Google Cloud.

* Assisted the organization in creating roadmaps that would guide them in successfully accomplishing their **data migration** objectives.

* I actively collaborated with key stakeholders to understand the organization's **data requirements** and challenges. Through in-depth analysis and discussions, I contributed to defining effective data strategies for achieving the data migration goals.

---

## Skills

---

### Soft

- Customer-oriented
- Team Work
- Problem solving

### Technical

- **Database modeling**: 
  - Operational 
  - Data Warehouse
  - Data Lake.
  - Data quality
- **Relational databases**: 
  - Microsoft SQL Server
  - MySQL
  - PostgreSQL.
- **Data Warehouse databases**: 
  - Snowflake
- **General purpose languages**: 
  - Python
  - SQL
- **Data processing Tools**:
  - Spark (PySpark)
  - Pandas (Python)
- **NoSQL databases**: 
  - MongoDB
- **Container development**: 
  - Docker
- **Infrastructure as a Code**:
  - Cloudformation
- **Version control**: 
  - Git
- **Cloud Providers**:
  - Amazon Web Services: S3, Lambda Functions, Batch, DynamoDB, Registry.
  - Google Cloud : cloud storage, cloud functions, BigQuery.

## Certifications
- [MongoDB SI Associate Certification Program]
- [Snowflake hands on Data Warehouse]

[//]: # ()

[MongoDB SI Associate Certification Program]: <https://learn.mongodb.com/c/qfHItPBNSh-zIGLhYnal6w>
[Snowflake hands on Data Warehouse]: <https://sgq.io/azg1xMV>
[Resume engineering]: <https://resume.io/resume-examples/engineering>